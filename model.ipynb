{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 2019.5.1\n",
    "\n",
    "@author: wujin\n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, log_dir, ckpt_dir, load_ckpt, image_height, image_width, image_channel=3):\n",
    "        self.log_dir = log_dir\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.image_channel = image_channel\n",
    "        self.sess = tf.Session()\n",
    "        self.trained_step = 0\n",
    "\n",
    "        self.inputs = tf.placeholder(tf.float32, shape=[None, image_height, image_width, image_channel])\n",
    "        self.targets = tf.placeholder(tf.float32, shape=[None, image_height, image_width, image_channel])\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        self.g = self.create_generator(self.inputs, self.image_channel, self.is_training)\n",
    "        self.d_fake = self.create_discriminator(self.inputs, self.g, self.is_training)\n",
    "        self.d_real = self.create_discriminator(self.inputs, self.targets, self.is_training, reuse=True)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        if load_ckpt:\n",
    "            self.trained_step = self.load()\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def lrelu(input_, leak=0.2):\n",
    "        with tf.name_scope('lrelu'):\n",
    "            return tf.maximum(input_, leak * input_)\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_norm(input_, is_training):\n",
    "        with tf.name_scope('batchnorm'):\n",
    "            return tf.contrib.layers.batch_norm(input_, is_training=is_training)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv(input_, output_channels, filter_size=4, stride=2, stddev=3e-2):\n",
    "        with tf.variable_scope('conv'):\n",
    "            in_channels = input_.get_shape()[-1]\n",
    "            filter_ = tf.get_variable(\n",
    "                name='filter',\n",
    "                shape=[filter_size, filter_size, in_channels, output_channels],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "            )\n",
    "            conv = tf.nn.conv2d(input_, filter_, [1, stride, stride, 1], padding='SAME')\n",
    "            return conv\n",
    "\n",
    "    @staticmethod\n",
    "    def deconv(input_, out_height, out_width, out_channels, filter_size=4, stride=2, stddev=3e-2):\n",
    "        with tf.variable_scope(\"deconv\"):\n",
    "            in_channels = input_.get_shape().as_list()[-1]\n",
    "            filter_ = tf.get_variable(\n",
    "                name='filter',\n",
    "                shape=[filter_size, filter_size, out_channels, in_channels],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "            )\n",
    "\n",
    "            batch_dynamic = tf.shape(input_)[0]\n",
    "            output_shape = tf.stack([batch_dynamic, out_height, out_width, out_channels])\n",
    "            conv = tf.nn.conv2d_transpose(input_, filter_, output_shape, [1, stride, stride, 1], padding=\"SAME\")\n",
    "            conv = tf.reshape(conv, [-1, out_height, out_width, out_channels])\n",
    "            return conv\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_edges(images):\n",
    "        def blur(image):\n",
    "            return cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "        def canny_otsu(image):\n",
    "            scale_factor = 255\n",
    "            scaled_image = np.uint8(image * scale_factor)\n",
    "\n",
    "            otsu_threshold = cv2.threshold(\n",
    "                cv2.cvtColor(scaled_image, cv2.COLOR_RGB2GRAY), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[0]\n",
    "            lower_threshold = max(0, int(otsu_threshold * 0.5))\n",
    "            upper_threshold = min(255, int(otsu_threshold))\n",
    "            edges = cv2.Canny(scaled_image, lower_threshold, upper_threshold)\n",
    "            edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            return np.float32(edges) * (1 / scale_factor)\n",
    "\n",
    "        blurred = [blur(image) for image in images]\n",
    "        canny_applied = [canny_otsu(image) for image in blurred]\n",
    "\n",
    "        return canny_applied\n",
    "\n",
    "    @staticmethod\n",
    "    def create_generator(input_, generator_output_channels, is_training):\n",
    "        class Encoder:\n",
    "            def __init__(self, name, out_channels, is_training, use_batch_norm=True):\n",
    "                self.name = name\n",
    "                self.out_channels = out_channels\n",
    "                self.is_training = is_training\n",
    "                self.use_batch_norm = use_batch_norm\n",
    "                self.in_height = None\n",
    "                self.in_width = None\n",
    "                self.output = None\n",
    "\n",
    "            def encode(self, input_):\n",
    "                with tf.variable_scope(self.name):\n",
    "                    output = Model.conv(input_, self.out_channels)\n",
    "                    if self.use_batch_norm:\n",
    "                        output = Model.batch_norm(output, self.is_training)\n",
    "                    output = Model.lrelu(output)\n",
    "\n",
    "                    input_shape = input_.get_shape().as_list()\n",
    "                    self.in_height = input_shape[1]\n",
    "                    self.in_width = input_shape[2]\n",
    "                    self.output = output\n",
    "                    return output\n",
    "\n",
    "        class Decoder:\n",
    "            def __init__(self, name, out_channels, is_training, use_batch_norm=True, dropout=None):\n",
    "                self.name = name\n",
    "                self.out_channels = out_channels\n",
    "                self.is_training = is_training\n",
    "                self.use_batch_norm = use_batch_norm\n",
    "                self.dropout = dropout\n",
    "                self.output = None\n",
    "\n",
    "            def decode(self, input_, out_height, out_width, skip_input=None):\n",
    "                with tf.variable_scope(self.name):\n",
    "                    if skip_input is None:\n",
    "                        merged_input = input_\n",
    "                    else:\n",
    "                        merged_input = tf.concat([input_, skip_input], axis=3)\n",
    "\n",
    "                    output = Model.deconv(merged_input, out_height, out_width, self.out_channels)\n",
    "                    if self.use_batch_norm:\n",
    "                        output = Model.batch_norm(output, self.is_training)\n",
    "                    output = tf.nn.relu(output)\n",
    "                    if self.dropout:\n",
    "                        output = tf.nn.dropout(output, keep_prob=1 - self.dropout)\n",
    "\n",
    "                    self.output = output\n",
    "                    return output\n",
    "\n",
    "        with tf.variable_scope('generator'):\n",
    "            ngf = 64\n",
    "\n",
    "            encoders = [\n",
    "                Encoder('encoder_0', ngf * 1, is_training, use_batch_norm=False),\n",
    "                Encoder('encoder_1', ngf * 2, is_training),\n",
    "                Encoder('encoder_2', ngf * 4, is_training),\n",
    "                Encoder('encoder_3', ngf * 8, is_training),\n",
    "                Encoder('encoder_4', ngf * 8, is_training),\n",
    "                Encoder('encoder_5', ngf * 8, is_training),\n",
    "                Encoder('encoder_6', ngf * 8, is_training),\n",
    "                Encoder('encoder_7', ngf * 8, is_training),\n",
    "            ]\n",
    "\n",
    "            for i, encoder in enumerate(encoders):\n",
    "                if i == 0:\n",
    "                    encoder_input = input_\n",
    "                else:\n",
    "                    encoder_input = encoders[i - 1].output\n",
    "                encoders[i].encode(encoder_input)\n",
    "\n",
    "            decoders = [\n",
    "                Decoder('decoder_0', ngf * 8, is_training, dropout=0.5),\n",
    "                Decoder('decoder_1', ngf * 8, is_training, dropout=0.5),\n",
    "                Decoder('decoder_2', ngf * 8, is_training, dropout=0.5),\n",
    "                Decoder('decoder_3', ngf * 8, is_training),\n",
    "                Decoder('decoder_4', ngf * 4, is_training),\n",
    "                Decoder('decoder_5', ngf * 2, is_training),\n",
    "                Decoder('decoder_6', ngf * 1, is_training),\n",
    "                Decoder('decoder_7', generator_output_channels, is_training),\n",
    "            ]\n",
    "\n",
    "            for i, decoder in enumerate(decoders):\n",
    "                if i == 0:\n",
    "                    decoder_input = encoders[-1].output\n",
    "                    decoder_skip_input = None\n",
    "                else:\n",
    "                    decoder_input = decoders[i - 1].output\n",
    "                    decoder_skip_input = encoders[-i - 1].output\n",
    "\n",
    "                decoders[i].decode(decoder_input, encoders[-i - 1].in_height, encoders[-i - 1].in_width, decoder_skip_input)\n",
    "\n",
    "            return tf.nn.tanh(decoders[-1].output)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_discriminator(input_, target, is_training, reuse=False):\n",
    "        class Layer:\n",
    "            def __init__(self, name, output_channels, stride, is_training, use_batch_norm=True, use_activation=True):\n",
    "                self.name = name\n",
    "                self.output_channels = output_channels\n",
    "                self.stride = stride\n",
    "                self.is_training = is_training\n",
    "                self.use_batch_norm = use_batch_norm\n",
    "                self.use_activation = use_activation\n",
    "                self.output = None\n",
    "\n",
    "            def conv(self, input_):\n",
    "                with tf.variable_scope(self.name):\n",
    "                    output = Model.conv(input_, self.output_channels, stride=self.stride)\n",
    "                    if self.use_batch_norm:\n",
    "                        output = Model.batch_norm(output, self.is_training)\n",
    "                    if self.use_activation:\n",
    "                        output = Model.lrelu(output)\n",
    "\n",
    "                    self.output = output\n",
    "                    return output\n",
    "\n",
    "        with tf.variable_scope('discriminator') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "\n",
    "            ndf = 64\n",
    "\n",
    "            layers = [\n",
    "                Layer('layer_0', ndf * 1, 2, is_training, use_batch_norm=False),\n",
    "                Layer('layer_1', ndf * 2, 2, is_training),\n",
    "                Layer('layer_2', ndf * 4, 2, is_training),\n",
    "                Layer('layer_3', ndf * 8, 1, is_training),\n",
    "                Layer('layer_4', 1, 1, is_training, use_batch_norm=False, use_activation=False),\n",
    "            ]\n",
    "\n",
    "            for i, layer in enumerate(layers):\n",
    "                if i == 0:\n",
    "                    layer_input = tf.concat([input_, target], axis=3)\n",
    "                else:\n",
    "                    layer_input = layers[i - 1].output\n",
    "                layers[i].conv(layer_input)\n",
    "\n",
    "            return tf.nn.sigmoid(layers[-1].output)\n",
    "\n",
    "    def train(self, training_image, test_image, total_epoch, steps_per_epoch, learning_rate, l1_weight, beta1, load_ckpt):\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        loss_g_gan = tf.reduce_mean(-tf.log(self.d_fake + epsilon))\n",
    "        loss_g_l1 = l1_weight * tf.reduce_mean(tf.abs(self.targets - self.g))\n",
    "        loss_g = loss_g_gan + loss_g_l1\n",
    "\n",
    "        loss_d_real = tf.reduce_mean(-tf.log(self.d_real + epsilon))\n",
    "        loss_d_fake = tf.reduce_mean(-tf.log(tf.ones_like(self.d_fake) - self.d_fake + epsilon))\n",
    "        loss_d = loss_d_real + loss_d_fake\n",
    "\n",
    "        vars_g = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "        vars_d = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "\n",
    "        # update batch_norm moving_mean, moving_variance\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_g = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(loss_g, var_list=vars_g)\n",
    "            train_d = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(loss_d, var_list=vars_d)\n",
    "\n",
    "        tf.summary.image('training_truth', self.targets, 4)\n",
    "        tf.summary.image('training_input', self.inputs, 4)\n",
    "        tf.summary.image('training_output', self.g, 4)\n",
    "\n",
    "        tf.summary.histogram('D_real', self.d_real)\n",
    "        tf.summary.histogram('D_fake', self.d_fake)\n",
    "\n",
    "        tf.summary.scalar('G_loss', loss_g)\n",
    "        tf.summary.scalar('G_loss_gan', loss_g_gan)\n",
    "        tf.summary.scalar('G_loss_l1', loss_g_l1)\n",
    "        tf.summary.scalar('D_loss', loss_d)\n",
    "        tf.summary.scalar('D_loss_real', loss_d_real)\n",
    "        tf.summary.scalar('D_loss_fake', loss_d_fake)\n",
    "\n",
    "        for var in vars_g:\n",
    "            tf.summary.histogram(var.name, var)\n",
    "        for var in vars_d:\n",
    "            tf.summary.histogram(var.name, var)\n",
    "\n",
    "        training_summary = tf.summary.merge_all()\n",
    "\n",
    "        test_summary_truth = tf.summary.image('test_truth', self.targets, 4)\n",
    "        test_summary_input = tf.summary.image('test_input', self.inputs, 4)\n",
    "        test_summary_output = tf.summary.image('test_output', self.g, 4)\n",
    "        test_summary = tf.summary.merge([test_summary_input, test_summary_output, test_summary_truth])\n",
    "\n",
    "        writer = tf.summary.FileWriter(self.log_dir, self.sess.graph)\n",
    "\n",
    "        # FIXME\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        if load_ckpt:\n",
    "            self.trained_step = self.load()\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=self.sess, coord=coord)\n",
    "\n",
    "        print('Training start')\n",
    "        for epoch in range(total_epoch):\n",
    "            for step in range(steps_per_epoch):\n",
    "                image_value = self.sess.run(training_image)\n",
    "                edges = self.detect_edges(image_value)\n",
    "\n",
    "                feed_dict = {self.inputs: edges, self.targets: image_value, self.is_training: True}\n",
    "                self.sess.run(train_d, feed_dict=feed_dict)\n",
    "                self.sess.run(train_g, feed_dict=feed_dict)\n",
    "                self.sess.run(train_g, feed_dict=feed_dict)\n",
    "\n",
    "                self.trained_step += 1\n",
    "                if self.trained_step % 100 == 0:\n",
    "                    print('step: {}'.format(self.trained_step))\n",
    "\n",
    "                    training_summary_value = self.sess.run(training_summary, feed_dict=feed_dict)\n",
    "                    writer.add_summary(training_summary_value, self.trained_step)\n",
    "\n",
    "                    image_value = self.sess.run(test_image)\n",
    "                    edges = self.detect_edges(image_value)\n",
    "\n",
    "                    feed_dict = {self.inputs: edges, self.targets: image_value, self.is_training: False}\n",
    "                    test_summary_value = self.sess.run(test_summary, feed_dict=feed_dict)\n",
    "                    writer.add_summary(test_summary_value, self.trained_step)\n",
    "\n",
    "                    if self.trained_step % 1000 == 0:\n",
    "                        self.save()\n",
    "\n",
    "        coord.join(threads)\n",
    "\n",
    "    def test(self, inputs):\n",
    "        output = self.sess.run(self.g, feed_dict={self.inputs: inputs, self.is_training: False})\n",
    "        return output\n",
    "\n",
    "    def save(self):\n",
    "        os.makedirs(self.ckpt_dir, exist_ok=True)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, self.ckpt_dir, global_step=self.trained_step)\n",
    "\n",
    "    def load(self):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.ckpt_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            trained_step = int(os.path.splitext(ckpt_name)[0][1:])\n",
    "\n",
    "            return trained_step\n",
    "        else:\n",
    "            return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
